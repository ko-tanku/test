"""
Test Material Main
ãƒ†ã‚¹ãƒˆè³‡æ–™ç”Ÿæˆã®ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import sys
import os
import logging
import time
from pathlib import Path
from typing import Dict, Any, List
import yaml

# sys.pathèª¿æ•´
project_root = Path(__file__).resolve().parents[3]
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

from src.core.config import PATHS
from src.core.utils import ensure_directory_exists
from .test_material_config import (
    get_test_config, get_execution_config, get_test_colors,
    get_test_chart_styles, get_test_table_styles
)
from .test_material_contents import TestMaterialContentManager
from .test_material_charts import run_chart_tests
from .test_material_tables import run_table_tests

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('test_material_generation.log', encoding='utf-8')
    ]
)

logger = logging.getLogger(__name__)


class TestMaterialMain:
    """
    ãƒ†ã‚¹ãƒˆè³‡æ–™ç”Ÿæˆã®ãƒ¡ã‚¤ãƒ³åˆ¶å¾¡ã‚¯ãƒ©ã‚¹
    """
    
    def __init__(self):
        """
        åˆæœŸåŒ–
        """
        self.config = get_test_config()
        self.execution_config = get_execution_config()
        self.colors = get_test_colors()
        self.chart_styles = get_test_chart_styles()
        self.table_styles = get_test_table_styles()
        
        # å¿…è¦ãªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
        self._setup_directories()
        
        # å®Ÿè¡Œçµæœã‚’è¨˜éŒ²
        self.execution_results = {
            "start_time": None,
            "end_time": None,
            "duration": None,
            "total_files_generated": 0,
            "test_results": {},
            "errors": []
        }
        
        logger.info("TestMaterialMain initialized")
    
    def _setup_directories(self) -> None:
        """
        å¿…è¦ãªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
        """
        try:
            directories = [
                PATHS["docs_dir"],
                PATHS["test_material_dir"],
                PATHS["test_material_assets_dir"],
                PATHS["test_material_charts_dir"],
                PATHS["test_material_tables_dir"]
            ]
            
            for directory in directories:
                ensure_directory_exists(directory)
                logger.info(f"Directory ensured: {directory}")
            
        except Exception as e:
            logger.error(f"Failed to setup directories: {e}")
            raise
    
    def _update_mkdocs_config(self) -> None:
        """
        mkdocs.yml ã‚’æ›´æ–°
        """
        try:
            mkdocs_config_path = PATHS["project_root"] / "mkdocs.yml"
            
            # åŸºæœ¬è¨­å®š
            mkdocs_config = {
                "site_name": self.config["material_name"],
                "site_description": self.config["description"],
                "site_author": self.config["author"],
                "docs_dir": "docs",
                "site_dir": "site",
                "theme": {
                    "name": "material",
                    "language": "ja",
                    "palette": {
                        "scheme": "default",
                        "primary": "green",
                        "accent": "orange"
                    },
                    "features": [
                        "navigation.tabs",
                        "navigation.top",
                        "navigation.tracking",
                        "search.suggest",
                        "search.highlight",
                        "search.share",
                        "toc.integrate",
                        "content.code.annotate",
                        "content.tooltips"
                    ]
                },
                "markdown_extensions": [
                    "admonition",
                    "pymdownx.details",
                    "pymdownx.superfences",
                    "pymdownx.highlight",
                    "pymdownx.tabbed",
                    "attr_list",
                    "md_in_html",
                    "footnotes",
                    "tables",
                    "toc",
                    "abbr",
                    "pymdownx.snippets"
                ],
                "plugins": [
                    "search"
                ],
                "nav" : {
                    "ãƒ›ãƒ¼ãƒ ": "index.md",
                    "ç¬¬1ç«  ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆæ¦‚è¦": "chapter_01_system_test_overview.md",
                    "ç¬¬2ç«  å›³è¡¨ç”Ÿæˆãƒ†ã‚¹ãƒˆ": "chapter_02_chart_generation_test.md",
                    "ç¬¬3ç«  è¡¨ç”Ÿæˆãƒ†ã‚¹ãƒˆ": "chapter_03_table_generation_test.md",
                    "ç¬¬4ç«  ç”¨èªç®¡ç†ãƒ†ã‚¹ãƒˆ": "chapter_04_knowledge_management_test.md",
                    "ç¬¬5ç«  çµ±åˆãƒ†ã‚¹ãƒˆ": "chapter_05_integration_test.md",
                    "ç”¨èªé›†": "glossary.md"
                },
            }
            
            # YAMLå½¢å¼ã§ä¿å­˜
            with open(mkdocs_config_path, 'w', encoding='utf-8') as f:
                yaml.dump(mkdocs_config, f, default_flow_style=False, allow_unicode=True)
            
            logger.info(f"MkDocs config updated: {mkdocs_config_path}")
            
        except Exception as e:
            logger.error(f"Failed to update mkdocs.yml: {e}")
            raise
    
    def run_chart_tests(self) -> Dict[str, Any]:
        """
        å›³è¡¨ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ
        
        Returns:
            ãƒ†ã‚¹ãƒˆçµæœ
        """
        if not self.execution_config.get("run_chart_tests", True):
            logger.info("Chart tests skipped (disabled in config)")
            return {"status": "skipped"}
        
        try:
            logger.info("Starting chart tests...")
            results = run_chart_tests()
            
            # çµæœã®æ­£è¦åŒ–
            if "test_status" in results:
                results["status"] = results["test_status"]
            
            if results.get("status") == "success" or results.get("test_status") == "success":
                logger.info("âœ… Chart tests completed successfully")
            else:
                logger.warning("âš ï¸ Chart tests completed with issues")
            
            return results
            
        except Exception as e:
            error_msg = f"Chart tests failed: {e}"
            logger.error(error_msg)
            return {"status": "failure", "error": error_msg}

    def run_table_tests(self) -> Dict[str, Any]:
        """
        è¡¨ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ
        
        Returns:
            ãƒ†ã‚¹ãƒˆçµæœ
        """
        if not self.execution_config.get("run_table_tests", True):
            logger.info("Table tests skipped (disabled in config)")
            return {"status": "skipped"}
        
        try:
            logger.info("Starting table tests...")
            results = run_table_tests()
            
            # çµæœã®æ­£è¦åŒ–
            if "test_status" in results:
                results["status"] = results["test_status"]
            
            if results.get("status") == "success" or results.get("test_status") == "success":
                logger.info("âœ… Table tests completed successfully")
            else:
                logger.warning("âš ï¸ Table tests completed with issues")
            
            return results
            
        except Exception as e:
            error_msg = f"Table tests failed: {e}"
            logger.error(error_msg)
            return {"status": "failure", "error": error_msg}    
    def run_content_generation(self) -> Dict[str, Any]:
        """
        ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆã‚’å®Ÿè¡Œ
        
        Returns:
            ç”Ÿæˆçµæœ
        """
        try:
            logger.info("Starting content generation...")
            
            # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚’åˆæœŸåŒ–
            content_manager = TestMaterialContentManager()
            
            # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ç”Ÿæˆ
            generated_files = content_manager.generate_content()
            
            # ãƒ†ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
            report_path = content_manager.generate_test_report()
            generated_files.append(report_path)
            
            logger.info(f"âœ… Content generation completed: {len(generated_files)} files generated")
            
            return {
                "status": "success",
                "generated_files": [str(f) for f in generated_files],
                "file_count": len(generated_files),
                "material_stats": content_manager.get_material_statistics()
            }
            
        except Exception as e:
            error_msg = f"Content generation failed: {e}"
            logger.error(error_msg)
            return {"status": "failure", "error": error_msg}
    
    def run_integration_tests(self) -> Dict[str, Any]:
        """
        çµ±åˆãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ
        
        Returns:
            ãƒ†ã‚¹ãƒˆçµæœ
        """
        if not self.execution_config.get("run_integration_tests", True):
            logger.info("Integration tests skipped (disabled in config)")
            return {"status": "skipped"}
        
        try:
            logger.info("Starting integration tests...")
            
            # ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª
            test_results = {
                "markdown_files": 0,
                "chart_files": 0,
                "table_files": 0,
                "total_files": 0,
                "missing_files": [],
                "file_sizes": {}
            }
            
            # Markdownãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºèª
            if PATHS["test_material_dir"].exists():
                md_files = list(PATHS["test_material_dir"].glob("*.md"))
                test_results["markdown_files"] = len(md_files)
                
                for md_file in md_files:
                    if md_file.exists():
                        test_results["file_sizes"][str(md_file)] = md_file.stat().st_size
                    else:
                        test_results["missing_files"].append(str(md_file))
            
            # å›³è¡¨ãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºèª
            if PATHS["test_material_charts_dir"].exists():
                chart_files = list(PATHS["test_material_charts_dir"].glob("*.html"))
                test_results["chart_files"] = len(chart_files)
                
                for chart_file in chart_files:
                    if chart_file.exists():
                        test_results["file_sizes"][str(chart_file)] = chart_file.stat().st_size
                    else:
                        test_results["missing_files"].append(str(chart_file))
            
            # è¡¨ãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºèª
            if PATHS["test_material_tables_dir"].exists():
                table_files = list(PATHS["test_material_tables_dir"].glob("*.html"))
                test_results["table_files"] = len(table_files)
                
                for table_file in table_files:
                    if table_file.exists():
                        test_results["file_sizes"][str(table_file)] = table_file.stat().st_size
                    else:
                        test_results["missing_files"].append(str(table_file))
            
            test_results["total_files"] = (
                test_results["markdown_files"] + 
                test_results["chart_files"] + 
                test_results["table_files"]
            )
            
            # çµ±åˆãƒ†ã‚¹ãƒˆè©•ä¾¡
            if test_results["missing_files"]:
                logger.warning(f"âš ï¸ Integration tests completed with missing files: {len(test_results['missing_files'])}")
                test_results["status"] = "partial_success"
            else:
                logger.info("âœ… Integration tests completed successfully")
                test_results["status"] = "success"
            
            return test_results
            
        except Exception as e:
            error_msg = f"Integration tests failed: {e}"
            logger.error(error_msg)
            return {"status": "failure", "error": error_msg}
    
    def generate_performance_report(self) -> Dict[str, Any]:
        """
        ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
        
        Returns:
            ãƒ¬ãƒãƒ¼ãƒˆçµæœ
        """
        if not self.execution_config.get("generate_performance_report", True):
            logger.info("Performance report generation skipped (disabled in config)")
            return {"status": "skipped"}
        
        try:
            logger.info("Generating performance report...")
            
            # å®Ÿè¡Œæ™‚é–“ã®è¨ˆç®—ï¼ˆå®‰å…¨ãªå‡¦ç†ï¼‰
            duration = self.execution_results.get("duration", 0)
            if duration is None:
                duration = 0
            
            # ãƒ•ã‚¡ã‚¤ãƒ«çµ±è¨ˆã®åé›†
            total_files = self.execution_results.get("total_files_generated", 0)
            if total_files is None:
                total_files = 0
            
            file_stats = {
                "total_files": total_files,
                "avg_generation_time": duration / max(1, total_files),
                "files_per_second": total_files / max(0.1, duration)
            }
            
            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®æ¦‚ç®—ï¼ˆç°¡æ˜“çš„ãªè¨ˆç®—ï¼‰
            try:
                import psutil
                process = psutil.Process()
                memory_info = process.memory_info()
                
                performance_data = {
                    "execution_time": duration,
                    "memory_usage": {
                        "rss": memory_info.rss / 1024 / 1024,  # MB
                        "vms": memory_info.vms / 1024 / 1024   # MB
                    },
                    "file_statistics": file_stats,
                    "system_info": {
                        "cpu_count": psutil.cpu_count(),
                        "cpu_percent": psutil.cpu_percent(),
                        "memory_percent": psutil.virtual_memory().percent
                    }
                }
            except ImportError:
                logger.warning("psutil not available, skipping system info")
                performance_data = {
                    "execution_time": duration,
                    "file_statistics": file_stats,
                    "system_info": "psutil not available"
                }
            
            logger.info("âœ… Performance report generated successfully")
            
            return {
                "status": "success",
                "performance_data": performance_data
            }
            
        except Exception as e:
            error_msg = f"Performance report generation failed: {e}"
            logger.error(error_msg)
            return {"status": "failure", "error": error_msg}    
        

    def cleanup_if_requested(self) -> None:
        """
        è¨­å®šã«å¿œã˜ã¦ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚’å®Ÿè¡Œ
        """
        if not self.execution_config.get("cleanup_after_test", False):
            logger.info("Cleanup skipped (disabled in config)")
            return
        
        try:
            logger.info("Starting cleanup...")
            
            # ãƒ†ã‚¹ãƒˆç”¨ãƒ•ã‚¡ã‚¤ãƒ«ã®å‰Šé™¤
            cleanup_patterns = [
                PATHS["test_material_charts_dir"] / "test_*.html",
                PATHS["test_material_tables_dir"] / "test_*.html"
            ]
            
            cleaned_files = 0
            for pattern in cleanup_patterns:
                for file_path in pattern.parent.glob(pattern.name):
                    if file_path.exists():
                        file_path.unlink()
                        cleaned_files += 1
            
            logger.info(f"âœ… Cleanup completed: {cleaned_files} files removed")
            
        except Exception as e:
            logger.error(f"Cleanup failed: {e}")
    
    def run_full_test_suite(self) -> Dict[str, Any]:
        """
        å®Œå…¨ãªãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆã‚’å®Ÿè¡Œ
        
        Returns:
            æœ€çµ‚å®Ÿè¡Œçµæœ
        """
        try:
            # å®Ÿè¡Œé–‹å§‹æ™‚åˆ»
            self.execution_results["start_time"] = time.time()
            
            logger.info("=" * 60)
            logger.info("ğŸš€ Starting MkDocs Materials Generator Test Suite")
            logger.info("=" * 60)
            
            # MkDocsè¨­å®šã‚’æ›´æ–°
            self._update_mkdocs_config()
            
            # å„ãƒ†ã‚¹ãƒˆã‚’é †æ¬¡å®Ÿè¡Œ
            test_phases = [
                ("Chart Tests", self.run_chart_tests),
                ("Table Tests", self.run_table_tests),
                ("Content Generation", self.run_content_generation),
                ("Integration Tests", self.run_integration_tests),
                ("Performance Report", self.generate_performance_report)
            ]
            
            for phase_name, phase_func in test_phases:
                logger.info(f"\nğŸ“‹ Phase: {phase_name}")
                logger.info("-" * 40)
                
                try:
                    result = phase_func()
                    self.execution_results["test_results"][phase_name] = result
                    
                    # ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã‚’ç´¯è¨ˆ
                    if "file_count" in result:
                        self.execution_results["total_files_generated"] += result["file_count"]
                    elif "generated_files" in result:
                        self.execution_results["total_files_generated"] += len(result["generated_files"])
                    
                    logger.info(f"âœ… {phase_name} completed")
                    
                except Exception as e:
                    error_msg = f"{phase_name} failed: {e}"
                    logger.error(f"âŒ {error_msg}")
                    self.execution_results["errors"].append(error_msg)
                    self.execution_results["test_results"][phase_name] = {
                        "status": "failure",
                        "error": error_msg
                    }
            
            # å®Ÿè¡Œçµ‚äº†æ™‚åˆ»
            self.execution_results["end_time"] = time.time()
            self.execution_results["duration"] = (
                self.execution_results["end_time"] - self.execution_results["start_time"]
            )
            
            # æœ€çµ‚çµæœã®è©•ä¾¡ï¼ˆå®Œå…¨ä¿®æ­£ç‰ˆï¼‰
            total_phases = len(test_phases)
            successful_phases = 0
            
            for phase_name, result in self.execution_results["test_results"].items():
                # è¤‡æ•°ã®æˆåŠŸåˆ¤å®šæ¡ä»¶ã‚’ãƒã‚§ãƒƒã‚¯
                is_success = False
                
                # 1. ç›´æ¥çš„ãªæˆåŠŸã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹
                if result.get("status") == "success" or result.get("test_status") == "success":
                    is_success = True
                
                # 2. éƒ¨åˆ†çš„æˆåŠŸã‚‚æˆåŠŸã¨ã—ã¦æ‰±ã†
                elif result.get("status") == "partial_success" or result.get("test_status") == "partial_success":
                    is_success = True
                
                # 3. ãƒ•ã‚¡ã‚¤ãƒ«ç”ŸæˆãŒæˆåŠŸã—ã¦ã„ã‚‹å ´åˆ
                elif result.get("file_count", 0) > 0 or len(result.get("generated_files", [])) > 0:
                    is_success = True
                
                # 4. æ¤œè¨¼çµæœã§æœ‰åŠ¹ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹å ´åˆ
                elif "validation" in result and result["validation"].get("valid_files", 0) > 0:
                    is_success = True
                
                # 5. ã‚¨ãƒ©ãƒ¼ãŒç„¡ã„å ´åˆï¼ˆçµ±åˆãƒ†ã‚¹ãƒˆã‚„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆï¼‰
                elif "error" not in result and result.get("status") != "failure":
                    is_success = True
                
                if is_success:
                    successful_phases += 1
                    logger.info(f"âœ… {phase_name} evaluated as successful")
                else:
                    logger.warning(f"âŒ {phase_name} evaluated as failed")
            
            success_rate = (successful_phases / total_phases) * 100
            
            # æœ€çµ‚ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º
            logger.info("\n" + "=" * 60)
            logger.info("ğŸ“Š Test Suite Execution Summary")
            logger.info("=" * 60)
            logger.info(f"ğŸ“ Total Files Generated: {self.execution_results['total_files_generated']}")
            logger.info(f"â±ï¸ Total Execution Time: {self.execution_results['duration']:.2f} seconds")
            logger.info(f"âœ… Successful Phases: {successful_phases}/{total_phases}")
            logger.info(f"ğŸ“ˆ Success Rate: {success_rate:.1f}%")
            
            if self.execution_results["errors"]:
                logger.info(f"âŒ Errors Encountered: {len(self.execution_results['errors'])}")
                for error in self.execution_results["errors"]:
                    logger.error(f"  - {error}")
            
            # æœ€çµ‚ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’æ±ºå®š
            if success_rate >= 80:
                final_status = "success"
                logger.info("ğŸ‰ Test Suite PASSED")
            elif success_rate >= 60:
                final_status = "partial_success"
                logger.info("âš ï¸ Test Suite PARTIALLY PASSED")
            else:
                final_status = "failure"
                logger.info("ğŸ’¥ Test Suite FAILED")
            
            # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            self.cleanup_if_requested()
            
            logger.info("=" * 60)
            
            # æœ€çµ‚çµæœã‚’è¿”ã™
            return {
                "final_status": final_status,
                "success_rate": success_rate,
                "execution_results": self.execution_results,
                "summary": {
                    "total_files": self.execution_results["total_files_generated"],
                    "duration": self.execution_results["duration"],
                    "successful_phases": successful_phases,
                    "total_phases": total_phases,
                    "error_count": len(self.execution_results["errors"])
                }
            }
            
        except Exception as e:
            logger.error(f"ğŸ’¥ Test suite execution failed: {e}")
            return {
                "final_status": "failure",
                "success_rate": 0,
                "error": str(e),
                "execution_results": self.execution_results
            }
        
    def print_detailed_report(self, results: Dict[str, Any]) -> None:
        """
        è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆã‚’å‡ºåŠ›
        
        Args:
            results: å®Ÿè¡Œçµæœ
        """
        print("\n" + "=" * 80)
        print("ğŸ“„ DETAILED EXECUTION REPORT")
        print("=" * 80)
        
        # åŸºæœ¬æƒ…å ±
        print(f"ğŸ·ï¸  Material Name: {self.config['material_name']}")
        print(f"ğŸ“… Generated At: {time.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"ğŸ¯ Final Status: {results['final_status'].upper()}")
        print(f"ğŸ“Š Success Rate: {results['success_rate']:.1f}%")
        
        # å®Ÿè¡Œçµ±è¨ˆ
        summary = results.get("summary", {})
        print(f"\nğŸ“ˆ EXECUTION STATISTICS")
        print(f"   Total Files Generated: {summary.get('total_files', 0)}")
        print(f"   Execution Time: {summary.get('duration', 0):.2f} seconds")
        print(f"   Successful Phases: {summary.get('successful_phases', 0)}/{summary.get('total_phases', 0)}")
        print(f"   Errors: {summary.get('error_count', 0)}")
        
        # å„ãƒ•ã‚§ãƒ¼ã‚ºã®è©³ç´°
        print(f"\nğŸ” PHASE Details")
        test_results = results.get("execution_results", {}).get("test_results", {})
        
        for phase_name, phase_result in test_results.items():
            status = phase_result.get("status", "unknown")
            status_icon = {"success": "âœ…", "partial_success": "âš ï¸", "failure": "âŒ", "skipped": "â­ï¸"}.get(status, "â“")
            
            print(f"   {status_icon} {phase_name}: {status.upper()}")
            
            if "file_count" in phase_result:
                print(f"      Files Generated: {phase_result['file_count']}")
            
            if "validation" in phase_result:
                validation = phase_result["validation"]
                print(f"      Valid Files: {validation.get('valid_files', 0)}/{validation.get('total_files', 0)}")
            
            if "error" in phase_result:
                print(f"      Error: {phase_result['error']}")
        
        # ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§
        print(f"\nğŸ“ GENERATED FILES")
        all_files = []
        for phase_result in test_results.values():
            if "generated_files" in phase_result:
                all_files.extend(phase_result["generated_files"])
        
        if all_files:
            print(f"   Total: {len(all_files)} files")
            for file_path in sorted(all_files):
                file_name = Path(file_path).name
                print(f"   ğŸ“„ {file_name}")
        else:
            print("   No files generated")
        
        # ã‚¨ãƒ©ãƒ¼è©³ç´°
        errors = results.get("execution_results", {}).get("errors", [])
        if errors:
            print(f"\nâŒ ERROR Details")
            for i, error in enumerate(errors, 1):
                print(f"   {i}. {error}")
        
        # æ¨å¥¨äº‹é …
        print(f"\nğŸ’¡ RECOMMENDATIONS")
        if results["success_rate"] >= 80:
            print("   ğŸ‰ Excellent! The system is working well.")
            print("   ğŸ’¼ Ready for production use.")
        elif results["success_rate"] >= 60:
            print("   âš ï¸ Good performance with some issues.")
            print("   ğŸ”§ Review and fix the reported errors.")
            print("   ğŸ§ª Consider running tests again after fixes.")
        else:
            print("   ğŸ’¥ Multiple issues detected.")
            print("   ğŸ› ï¸ Significant fixes required before production use.")
            print("   ğŸ“ Consider consulting the development team.")
        
        print("=" * 80)
    
    def save_results_to_file(self, results: Dict[str, Any], output_file: str = "test_results.json") -> None:
        """
        å®Ÿè¡Œçµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        
        Args:
            results: å®Ÿè¡Œçµæœ
            output_file: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«å
        """
        try:
            import json
            
            output_path = PATHS["test_material_dir"] / output_file
            
            # JSONã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚ºå¯èƒ½ãªå½¢å¼ã«å¤‰æ›
            serializable_results = {
                "metadata": {
                    "material_name": self.config["material_name"],
                    "generated_at": time.strftime('%Y-%m-%d %H:%M:%S'),
                    "version": "1.0.0"
                },
                "results": results
            }
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(serializable_results, f, indent=2, ensure_ascii=False)
            
            logger.info(f"ğŸ“ Results saved to: {output_path}")
            
        except Exception as e:
            logger.error(f"Failed to save results to file: {e}")


def main():
    """
    ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°
    """
    try:
        # ãƒ†ã‚¹ãƒˆãƒ¡ã‚¤ãƒ³ã‚’åˆæœŸåŒ–
        test_main = TestMaterialMain()
        
        # å®Œå…¨ãªãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆã‚’å®Ÿè¡Œ
        results = test_main.run_full_test_suite()
        
        # è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆã‚’å‡ºåŠ›
        test_main.print_detailed_report(results)
        
        # çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        test_main.save_results_to_file(results)
        
        # çµ‚äº†ã‚³ãƒ¼ãƒ‰ã‚’è¨­å®š
        exit_code = 0 if results["final_status"] == "success" else 1
        
        print(f"\nğŸ Test suite completed with exit code: {exit_code}")
        
        return exit_code
        
    except KeyboardInterrupt:
        logger.info("âŒ Test suite interrupted by user")
        return 130
    except Exception as e:
        logger.error(f"ğŸ’¥ Unexpected error: {e}")
        return 1


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)